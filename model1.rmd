---
title: "Project 1 - Phân loại chất lượng rượu vang"
output:
  html_document:
    df_print: paged
  pdf_document: default
date: "2024-06-25"
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
library(janitor)
library(tidyverse)
library(leaps) 
library(boot)
library(ggplot2)
library(nnet)
library(caret)
library(regclass)
library(pROC)
library(gridExtra)
library(pscl)
set.seed(21)
```

# Load datasets

```{r}
red_data = read.csv("datasets/winequality-red.csv") |> clean_names()
white_data = read.csv("datasets/winequality-white.csv") |> clean_names()

# add categorical varialbles to both sets
red_data['color'] = 1
white_data['color'] = 2

# merge red wine and white wine datasets
data = rbind(red_data, white_data)
data$color = factor(data$color)


reg_data = data |> select(-quality, quality)

#wine quality is turned into nominal data
data$segmentation = ifelse(data$quality < 6,'poor','excellent')
data$segmentation[data$quality==6] = 'normal'
data$segmentation = as.factor(data$segmentation)
data = subset(data, select = -quality)
class_data = data

```

```{r}
reg_data
class_data
```


# IV. BUILDING MODELS

## Train test split

```{r}
# get features and taget
features = colnames(data) 
features = subset(features, features != "quality")
target = "quality"

# function for train test split
train_test_split = function(data, test_size){
  set.seed(48)
  ind = sample(nrow(data), size = nrow(data) * test_size, replace = FALSE, prob = NULL)
  train_set = data[-ind,]
  test_set = data[ind,]
  
  return(list("train_set" = train_set, "test_set" = test_set))
}

# split datasets with test_size = 0.25
# on reg_data
traintest_regdata = train_test_split(reg_data, test_size = 0.25)
regdatatrain = traintest_regdata$train_set
regdatatest = traintest_regdata$test_set
# on class_data
traintest_classdata = train_test_split(class_data, test_size = 0.25)
classdatatrain = traintest_classdata$train_set
classdatatest = traintest_classdata$test_set
```

-   Plot the distribution of the quality variable for each dataset
```{r}
# Create the plots
plot1 <- ggplot(regdatatrain, aes(x = quality)) + 
  geom_histogram(bins = 10, color = "black", stat = "count") + 
  labs(x = "Quality", y = "Frequency", title = "Distribution of Quality (wine_train)")

plot2 <- ggplot(regdatatest, aes(x = quality)) + 
  geom_histogram(bins = 10, color = "black", stat = "count") + 
  labs(x = "Quality", y = "Frequency", title = "Distribution of Quality (wine_test)")

plot3 <- ggplot(classdatatrain, aes(x = segmentation)) + 
  geom_histogram(bins = 10, color = "black", stat = "count") + 
  labs(x = "Segmentation", y = "Frequency", title = "Distribution of Segmentation (wine_train)")

plot4 <- ggplot(classdatatest, aes(x = segmentation)) + 
  geom_histogram(bins = 10, color = "black", stat = "count") + 
  labs(x = "Segmentation", y = "Frequency", title = "Distribution of Segmentation (wine_test)")

# Arrange the plots in a grid
grid.arrange(grobs = list(plot1, plot2, plot3, plot4), nrow = 2)
```

## Scale data
```{r}
scale_data <- function(train_data, test_data, exclude_cols) {
  # Get the column indices to exclude
  exclude_cols_idx <- which(names(train_data) %in% exclude_cols)
  
  # Scale the training data
  train_scaled <- scale(train_data[,-exclude_cols_idx])
  
  # Scale the testing data using the same scaling parameters
  test_scaled <- scale(test_data[,-exclude_cols_idx], 
                       center = attr(train_scaled, "scaled:center"), 
                       scale = attr(train_scaled, "scaled:scale"))
  
  # Combine the scaled data with the excluded columns
  train_scaled <- as.data.frame(cbind(train_data[, exclude_cols_idx], train_scaled))
  test_scaled <- as.data.frame(cbind(test_data[, exclude_cols_idx], test_scaled))
  
  return(list("train_scaled" = train_scaled, "test_scaled" = test_scaled))
}

# on reg_data
data_scaled = scale_data(regdatatrain, regdatatest, c("color", "quality"))
regdatatrain_scaled = data_scaled$train_scaled
regdatatest_scaled = data_scaled$test_scaled

# on class_data
classdata_scaled = scale_data(classdatatrain, classdatatest, c("color", "segmentation"))
classdatatrain_scaled = classdata_scaled$train_scaled
classdatatest_scaled = classdata_scaled$test_scaled
```


## 1. Linear Regression

### 1.1 Model selection

```{r}
#specify the cross-validation method
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
#fit a regression model and use k-fold CV to evaluate performance
md_lm_data <- train(quality ~., data = regdatatrain_scaled, method = "lm", trControl = train_control)
summary(md_lm_data)
```

```{r}
# Hàm kiểm định cho hệ số 
create_results_dataframe = function(model, boot_model, conf_level = 0.95) {
  # Tính ước lượng cho các hệ số từ mô hình hồi quy
  est = coef(model)
  
  # Tính độ lệch chuẩn của ước lượng hệ số từ bootstrap
  se = apply(boot_model$t, 2, sd)
  
  # Tính khoảng tin cậy 95% cho các hệ số từ bootstrap
  ci_95 = sapply(1:ncol(boot_model$t), function(i) {
    CI = boot.ci(boot_model, index = i, type = "perc", conf = conf_level)$percent[1, 4:5]
    paste0("(", round(CI[1], 2), ", ", round(CI[2], 2), ")")
  })
  
  # Tính p-value cho các hệ số từ bootstrap
  p_values = sapply(1:ncol(boot_model$t), function(x) {
    qt0 = mean(boot_model$t[, x] <= 0)
    if (qt0 < 0.5) {
      return(2*qt0)
    } else {
      return(2*(1 - qt0))
    }
  })
  
  # Tạo dataframe
  df_results = data.frame(
    Est = est,
    SE = se,
    CI_95 = ci_95,
    p_value = p_values,
    row.names = names(est)
  )
  
  # Vẽ histogram của các kết quả ước lượng bootstrap của hệ số
  nrow = 2
  ncol <- ceiling(length(est) / nrow)
  par(mfrow = c(nrow, ncol))
  for (i in 1:length(est)) {
    hist(boot_model$t[, i], main = names(est)[i], xlab = names(est)[i])
  }
  
  return(df_results)
}

boot_func = function(data, ind, formula, ...){
  data_new = data[ind,]
  model = lm(formula = formula, data = data_new, ...)
  return(model$coefficients)
}
```

### 1.2 Checking Multicollinarilty

In multicollinearity,collinearity exists between three or more variables
even if no pair of variables has a particularly high correlation. This
means that there is redundancy between predictor variables.

In the presence of multicollinearity, the solution of the regression
model becomes unstable.

Multicollinearity can assessed by computing a score called the variance
inflation factor (or VIF), which measures how much the variance of a
regression coefficient is inflated due to multicollinearity in the
model.

```{r}
linear_model1 = lm(quality ~., data = regdatatrain_scaled)
boot_linear_model1 = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, formula = quality ~.)

results_df1 = create_results_dataframe(model = linear_model1, boot_model = boot_linear_model1)
print(results_df1)
```

The VIF of a predictor is a measure for how easily it is predicted from
a linear regression using the other predictors.

```{r}
VIF(linear_model1)
```

We can see multicollinarity over here. We remove density cause it
exibits multicollinarity. Now fitting the new model.

```{r}
linear_model2 = lm(quality ~ . - density , data = regdatatrain_scaled)
boot_linear_model2 = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, formula = quality ~. - density )

results_df2 = create_results_dataframe(model = linear_model2, boot_model = boot_linear_model2)
print(results_df2)
```

We fit another model removing fixed_acidity

```{r}
linear_model3 = lm(quality ~ . -density -fixed_acidity, data = regdatatrain_scaled)
boot_linear_model3 = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, 
                          formula = quality ~. -density -fixed_acidity)

results_df3 = create_results_dataframe(model = linear_model3, boot_model = boot_linear_model3)
print(results_df3)
```

We fit another model removing p_h

```{r}
linear_model4 = lm(quality ~ . -density -fixed_acidity -p_h, data = regdatatrain_scaled)
boot_linear_model4 = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, 
                          formula = quality ~. -density -fixed_acidity -p_h)

results_df4 = create_results_dataframe(model = linear_model4, boot_model = boot_linear_model4)
print(results_df4)
```

We fit another model removing citric_acid

```{r}
linear_model5 = lm(quality ~ . -density -fixed_acidity -p_h -citric_acid, data = regdatatrain_scaled)
boot_linear_model5 = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, 
                          formula = quality ~. -density -fixed_acidity -p_h -citric_acid)

results_df5 = create_results_dataframe(model = linear_model5, boot_model = boot_linear_model5)
print(results_df5)
```

### 1.3 Why we selected linear_model5?
linear_model1
```{r}
linear_model1
```

linear_model5
```{r}
linear_model5
```

The main difference between the two models is that linear_model1
includes all the variables, while linear_model5 excludes density,
fixed_acidity, p_h and citric_acid.

The results show that linear_model5 explains about 28.18% of the
variation in quality, and when the excluded variables are added back in,
the explanatory power increases by only 0.58% to 28.76%. The question is
whether this 0.58% increase is statistically significant.

To answer this, you've performed hypothesis tests for each of the
excluded variables and obtained the following p-values: 
  + density: p = 0.1
  + fixed_acidity: p = 0.73
  + p_h: p = 0.24
  + citric_acid.: p = 0.36

Since all these p-values are greater than the typical significance level
of 0.05, we conclude that the excluded variables do not provide
additional information or improve the prediction of quality beyond
what's already explained by the variables in linear_model5. This
suggests that these variables do not have a significant impact on the
quality of red wine, and including them in the model does not
significantly improve its predictive power.

In other words, once the effects of these seven variables are accounted
for, the additional variables do not have a statistically significant
impact on the prediction of quality.

### 1.4 Perform inferential statistics

#### a. F-test

-   Thực hiện các thống kê suy luận cho mô hình linear_model5 vừa xác
    định được Ta áp dụng phương pháp bootstrap để ước lượng khoảng tin
    cậy và kiểm định giả thuyết βj = 0.
    “Có thực sự tồn tại mối liên hệ giữa biến Xj và Y ”.

```{r}
linear_model = lm(quality ~ . -density -fixed_acidity -p_h -citric_acid, data = regdatatrain_scaled)
boot_linear_model = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, 
                          formula = quality ~. -density -fixed_acidity -p_h -citric_acid)

summary(linear_model)
```

F- Statistic: p value < 2.2e -16

```{r}
print(boot_linear_model)
results_df = create_results_dataframe(model = linear_model, boot_model = boot_linear_model)
print(results_df)
```

We can see that p value is less than 0.05. Thus we can reject the null hypothesis and accept the alternate hypothesis. Thus,we can say that the model is significant.

#### b. ANOVA

```{r}
par(mfrow=c(2,2), oma = c(1,1,0,0) + 0.1, mar = c(3,3,1,1) + 0.1)
plot(linear_model)
```

-   Residuals vs Fitted plot shows if residuals have non-linear
    patterns.Residuals around a horizontal line without distinct
    patterns, that is a good indication we don’t have non-linear
    relationships.

-   Normal QQ plot shows residuals fitting the line. Hence, can call it
    norlly distibuted residuals.

-   Scale-Location plot shows if residuals are spread equally along the
    ranges of predictors. This is how you can check the assumption of
    equal variance (homoscedasticity). It’s good if you see a horizontal
    line with equally (randomly) spread points.

-   Residuals vs Leverage plot has a typical look when there is some
    influential case. You can barely see Cook’s distance lines (a red
    dashed line) because all cases are well inside of the Cook’s
    distance.

#### c. Kiểm định khoảng tin cậy 95% cho trung bình

Giả sử nồng độ phần trăm của các chất là

```{r}
head(regdatatest_scaled, 1)
```

Khi đó, dựa vào mô hình, ta có thể ước tính được trung bình chất lượng
rượu là khoảng 5.9 điểm

```{r}
new_data = head(regdatatest_scaled, 1)
pred_data = predict(linear_model, new_data)
pred_data
```

Và để tìm khoảng tin cậy cho giá trị trung bình chất lượng rượu, ta sử
dụng phương pháp bootstrap. Vì, ta đã chạy bootstrap ở phần trước với
1000 lần lặp để ước tính các hệ số, ta có thể sử dụng kết quả này để
tính các giá trị ước đoán trung bình chất lượng rượu trong mỗi lần lặp
mẫu:

```{r}
x_wine <- as.numeric(cbind(1,new_data[1,]))

y_wine <- apply(boot_linear_model$t, 1, function(x){x_wine %*% t(x) })
quantile(y_wine, probs = c(0.025, 0.975))
```

Như vậy, trung bình chất lượng rượu được tiên lượng theo mô hình này, có
giá trị thay đổi từ (-3.234217, 11.899066), với độ tin cậy 95%.

#### d. Ước lượng khoảng tiên đoán 95%

Ngoài ra, ta còn có thể sử dụng phương pháp bootstrap để ước lượng
khoảng tiên đoán cho chất lượng rượu, theo nồng độ các chất tan có trong
rượu, dựa trên mô hình:

```{r}
resid_data = residuals(linear_model)
y_wine_pd_pci = y_wine + sample(resid_data, size = 1000, replace = TRUE)
quantile(y_wine_pd_pci, probs = c(0.025, 0.975))

```

Như vậy, khoảng tiên đoán cho chất lượng rượu được tiên lượng theo mô
hình này, có giá trị thay đổi từ (3.75755, 12.58569), với độ tin cậy
95%.

### 1.5 Đánh giá mô hình

Actual
```{r}
head(regdatatest_scaled$quality)
```

Predict

```{r}
pred <- predict(linear_model, regdatatest_scaled)  
head(pred)
```

Round the numeric values to the nearest integer values.

```{r}
pred = round(pred)
head(pred)
```

```{r}
tst_tab <- table(predicted = pred, actual = regdatatest_scaled$quality)
tst_tab
```

```{r}
RMSE_value <- RMSE(pred, regdatatest_scaled$quality)
MAE_value <- MAE(pred, regdatatest_scaled$quality)
R_squared <- R2(pred, regdatatest_scaled$quality)
accuracy <- sum(diag(tst_tab))/length(regdatatest_scaled$quality)
# Hiển thị kết quả
cat("RMSE:", RMSE_value, "\n")
cat("MAE:", MAE_value, "\n")
cat("R-squared:", R_squared, "\n")
cat("Accuracy: ", accuracy,"\n")
```

### 1.6 Nhận xét và rút ra kết luận cho mô hình

```{r}
linear_model5
```

In particular, the bootstrap summary shows that the coefficients for
`residual_sugar`,`free_sulfur_dioxide`, `sulphates` and `alcohol` are
consistently positive across all bootstrap samples, indicating that
higher values of these variables are associated with higher quality data
wine. On the other hand, the coefficients for `total_sulfur_dioxide`,
`chlorides`, `volatile_acidity`, and `color` are consistently negative,
indicating that higher values of these variables are associated with
lower quality data wine.

If the winemaker wants to increase the quality of the data wine, it
would be beneficial to:

-   Increase the concentration of `residual_sugar`: Residual sugar can
    contribute to a wine's sweetness and body, which could enhance its
    flavor profile and overall quality.
-   Increase the concentration of `free_sulfur_dioxide`: Free sulfur
    dioxide is an antioxidant that can help to prevent oxidation and
    spoilage in wine. Increasing its concentration could help to improve
    the wine's freshness and overall quality.
-   Increase the concentration of `sulphates`: Sulphates are a type of
    preservative that can help to prevent spoilage and oxidation in
    wine. Increasing the concentration of sulphates could help to
    improve the overall quality and stability of the wine.
-   Increase the concentration of `alcohol`: Alcohol content is a
    critical factor in wine quality, as it affects the wine's body,
    flavor, and aging potential. Wines with optimal alcohol levels
    (typically between 12% and 15% ABV) tend to be more balanced and
    harmonious, while wines with excessive or insufficient alcohol can
    be unbalanced and lack character. Increasing the concentration of
    alcohol could help to enhance the wine's flavor profile and overall
    quality.

On the other hand, it would be beneficial to:

-   Decrease the concentration of `total_sulfur_dioxide`: While sulfur
    dioxide is an important preservative in wine, excessive levels can
    have negative effects on the wine's flavor and aroma. Decreasing the
    concentration of total sulfur dioxide could help to improve the
    wine's overall character and quality.
-   Decrease the concentration of `color`: While color is an important
    aspect of wine, excessive color intensity can be associated with
    lower quality wine. Decreasing the color intensity could help to
    improve the wine's overall quality.
-   Decrease the concentration of `chlorides`: Chlorides are a type of
    salt that can contribute to the wine's bitterness and astringency.
    Decreasing the concentration of chlorides could help to improve the
    wine's flavor profile and overall quality.
-   Decrease the concentration of `volatile_acidity`: Volatile acidity
    refers to the concentration of acids like acetic acid, which can
    contribute to the wine's aroma and flavor. While some volatile
    acidity is desirable, excessive levels can lead to off-flavors and
    aromas, negatively impacting wine quality.. Decreasing the
    concentration of volatile acidity could help to improve the wine's
    flavor profile and overall quality.

By adjusting these variables, the winemaker may be able to improve the
overall quality of the red wine. However, it's important to note that
wine production is a complex process, and changing these variables may
have unintended consequences on the wine's flavor, aroma, and overall
character. Additionally, there may be other factors that influence the
quality of the wine, such as grape variety, climate, and winemaking
techniques.

## 2. Stepwise regression

### 2.1 Model selection

#### a. Forward Stepwise Selection

```{r}
#define intercept-only model
intercept_only <- lm(quality ~ 1, data=regdatatrain_scaled)

#define model with all predictors
all <- lm(quality ~ ., data=regdatatrain_scaled)

#perform forward stepwise regression
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0, k=20)

#view results of forward stepwise regression
forward$anova
```

coefficients

```{r}
forward$coefficients
```

#### b. Backward Stepwise Selection

```{r}
#perform backward stepwise regression
backward <- step(all, direction='backward', scope=formula(all), trace=0, k=20)

#view results of backward stepwise regression
backward$anova
```

coefficients

```{r}
backward$coefficients
```

#### c. Both-Direction Stepwise Selection

```{r}
#perform backward stepwise regression
both <- step(intercept_only, direction='both', scope=formula(all), trace=0, k=20)

#view results of backward stepwise regression
both$anova
```

coefficients

```{r}
both$coefficients
```

### 2.2 Perform inferential statistics

```{r}
summary(both)
```

```{r}
both_model = lm(formula = quality ~ alcohol + volatile_acidity + sulphates + residual_sugar + color + density, data = regdatatrain_scaled)
boot_stepwise_model = boot(data = regdatatrain_scaled, statistic = boot_func, R = 1000, 
                           formula = quality ~ alcohol + volatile_acidity + sulphates + residual_sugar + color + density)

results_df_stepwise = create_results_dataframe(model = both_model, boot_model = boot_stepwise_model)
results_df_stepwise
```

-   stepwise regression: 
`quality` = 6.1 + 0.35alcohol - 0.26volatile_acidity + 0.08sulphates + 0.20residual_sugar - 0.37color - 0.15density

The results show that stepwise regression explains about 28.10% of the
variation in quality, and when the excluded variables are added back in,
the explanatory power increases by only 0.66% to 28.76%. The question is
whether this 0.66% increase is statistically significant.

To answer this, you've performed hypothesis tests for each of the
excluded variables and obtained the following p-values: 
  + fixed_acidity: p = 0.71 
  + free_sulfur_dioxide: p = 0.09 
  + citric_acid: p = 0.49 
  + chlorides: p = 0.12 
  + p_h: p = 0.45 
  + ...

Since all these p-values are greater than the typical significance level
of 0.05, we conclude that the excluded variables do not provide
additional information or improve the prediction of quality beyond
what's already explained by the variables in linear_model5, namely
volatile_acidity, citric_acid, chlorides, total_sulfur_dioxide, p_h,
sulphates, and alcohol.

### 2.3 Nhận xét

Based on 3 Direction Stepwise regression models, we identified the most
important variables that affect the quality of the red wine, which are:
`alcohol`, `residual_sugar`, `sulphates`, `color`, `volatile_acidity`,
`density`.

And more specifically, we found that: - `alcohol`, `residual_sugar` and
`sulphates` have a positive effect on wine quality, meaning that
increasing their concentration can improve wine quality. - `color`,
`volatile_acidity` and `density` have a negative impact on wine quality,
meaning that increasing their concentration can reduce wine quality.

This suggests that winemakers should focus on: - Optimizing the levels
of: + sulphates: to improve the wine's stability and overall quality +
alcohol: to enhance the wine's flavor profile and overall quality +
residual_sugar: to contribute to a wine's sweetness and body, which can
enhance its flavor profile and overall quality - Minimizing the levels
of: + color: to avoid negatively impacting the wine's quality +
volatile_acidity: to prevent an unpleasant, vinegary flavor + density:
to maintain a balanced and harmonious wine.

By understanding the relationship between these variables and wine
quality, winemakers can make informed decisions about how to optimize
their winemaking processes to produce quality wines. high quality.

It is worth noting that these findings can also be used to identify
potential areas for improvement in the winemaking process, such as
adjusting the fermentation process to optimize the levels of alcohol,
residual_sugar, and sulphates, and implementing techniques to minimize
the levels of color, volatile_acidity, and density. By doing so,
winemakers can refine their craft and produce higher-quality red wines
that meet the desired standards.

## 3. Multinominal Logistic

Xây dựng mô hình phân loại đa nhóm với mô hình multinominal logistic
trên dữ liệu `datatrain`

### 3.1 Model selection

```{r}
md_mlogit1 = multinom(segmentation~., data = classdatatrain_scaled, maxit = 1000)
```

Thuật toán hội tụ sau 30 lần lặp. Tổng hợp kết quả ước lượng mô hình
như sau:

```{r}
md_mlogit1
```

Calculate coefficient and standard error

```{r}
summary(md_mlogit1)
```

Interpretation of Train Model data. After we run the model, nnet reports
the summary iterations and declaration about model convergence. In this
case, the model converged quite after 30 iterations. Model execution
output shows some iteration history and includes the final negative
log-likelihood 4194. This value is multiplied by two as shown in the
model summary as the Residual Deviance

The Akaike Information Criterion (AIC) is 8441.024 and it provides a
method for assessing the quality of your model through comparison of
related models. If we have more than one similar candidate models (where
all of the variables of the simpler model occur in the more complex
models), then we should select the model that has the smallest AIC. It’s
useful for comparing models. However, unlike adjusted R-squared, the
number itself is not meaningful.

### 3.2 Perform inferential statistics

#### a. Calculate Z score and p-Value for the variables

```{r}
# Z score
mlogit_output = summary(md_mlogit1)
z <- mlogit_output$coefficients/mlogit_output$standard.errors
p <- (1-pnorm(abs(z),0,1))*2 
print(z, digits=2)
```

```{r}
print(p, digits=2)
```

The p-Value for quality tells us that citric acid and residual sugar
(for four levels of quality) are not significant. Now I’ll explore the
entire data set, and analyze if we can remove any variables which do not
add to model performance.

```{r}
Pquality5 <- rbind(mlogit_output$coefficients[2, ],mlogit_output$standard.errors[2, ],z[2, ],p[2, ])
rownames(Pquality5) <- c("Coefficient","Std. Errors","z stat","p value")
Pquality5 = as.data.frame(Pquality5)
Pquality5
```

#### b. Calculation of odds ratio- extract the coefficients from the model and exponentiate

```{r}
oddsML <- exp(coef(mlogit_output))
print(oddsML, digits =2)
```

The relative risk/odds ratio for a one-unit increase in the variable
residual_sugar is 0.44 for being in poor vs normal. The relative
risk/odds ratio for a one-unit increase in the variable alcohol is 0.71
for being in poor vs normal.

### 3.3 Calculation of important variables

Calculation of important variables using Caret function

```{r}
mostImportantVariables <- varImp(md_mlogit1)
mostImportantVariables$Variables <- row.names(mostImportantVariables)
mostImportantVariables <- mostImportantVariables[order(-mostImportantVariables$Overall),]
print(head(mostImportantVariables))
```

The importance of these features in classifying wine quality can be
attributed to their chemical and biological properties, which affect the
wine's overall character and perceived quality. Here's a brief
explanation for each feature:

-   Tier 1: High-impact features
  -  `density`: Density, or specific gravity, is a measure of the mass of a substance per unit volume. In wine, density is affected by factors such as sugar content, alcohol content, and acidity. Higher density wines may indicate higher sugar or alcohol content, which can contribute to a richer, fuller-bodied wine. Lower density wines may indicate lower sugar or alcohol content, which can result in a lighter, more delicate wine. Density is an important factor in wine classification, as it can provide insight into the wine's sweetness, body, and overall quality.
  - `residual_sugar`: Residual sugar is the sugar that remains in wine after fermentation is complete. It is a key factor in determining the wine's sweetness, which can have a significant impact on its perceived quality. Wines with higher residual sugar may be sweeter and richer, while wines with lower residual sugar may be drier and more austere. Residual sugar is an important predictor of wine quality, as it can provide insight into the wine's balance, complexity, and overall character.
  -   `color`: The color of wine is a critical aspect of its overall character and perceived quality. Wine color is influenced by the grape variety, skin contact, and aging processes. Different colors can indicate different flavor profiles, tannin levels, and aging potential. In wine classification, color is a key factor in distinguishing between different wine styles and quality levels.
  
-   Tier 2: Moderate-impact features
    -   `volatile_acidity`: Volatile acidity refers to the amount of
        acetic acid and other compounds that contribute to the wine's
        aroma and flavor. High levels of volatile acidity can give wine
        an unpleasant, vinegary taste. Moderate levels, on the other
        hand, can add complexity and character to the wine. In wine
        classification, volatile acidity is an important factor in
        determining the wine's quality and style.
    -   `alcohol`: Alcohol content is a critical factor in determining wine quality, as it can affect the wine's body, flavor, and aroma. Higher alcohol content can contribute to a fuller-bodied, more robust wine, while lower alcohol content can result in a lighter, more delicate wine. Alcohol content is also an important predictor of wine style, as different wine regions and grape varieties are known for producing wines with distinct alcohol levels.
    
-   Tier 3: Low-impact features
    -   `fixed_acidity`: Fixed acidity refers to the non-volatile acids
        present in wine, such as tartaric and malic acids. These acids
        play a crucial role in maintaining the wine's balance,
        freshness, and aging potential. In wine classification, fixed
        acidity is an important factor in determining the wine's quality
        and style.

These features are important because they influence the wine's chemical
and biological properties, which in turn affect its flavor, aroma, and
overall character. Understanding the importance of these features in predicting wine quality can help winemakers and wine experts to optimize their production processes and make more informed decisions about wine classification. By focusing on factors such as density, residual sugar, and color, winemakers can create wines that are more likely to be perceived as high-quality and appealing to consumers. Additionally, understanding the impact of alcohol and acidity on wine quality can help winemakers to create wines that are more balanced, stable, and age-worthy. Keep in mind that wine quality is a complex and multifaceted concept, and many other factors can influence it, including grape variety,
terroir, winemaking techniques, and more. However, these six features
provide a solid foundation for understanding the chemical and biological
aspects of wine quality.

### 3.4 Lean Multinomial Model

I used top 4 mostImportantVariables to select key variables for lean
model

```{r}
md_mlogit2 = multinom(segmentation ~ density + residual_sugar + color + alcohol, data = classdatatrain_scaled, maxit = 1000)
```

```{r}
summary(md_mlogit2)
```

### 3.5 Evaluation of Multinomial Logistics Models

```{r}
mlogit_ModelFit<- rbind(pscl::pR2(md_mlogit1)[1:6],pscl::pR2(md_mlogit2)[1:6])
```

```{r}
rownames(mlogit_ModelFit) <- c("Model-1", "Model-2")
print(mlogit_ModelFit, digits = 2)
```

### 3.6 Calculation of Multinomial Model error

```{r}
mlogit1_output = summary(md_mlogit1)
mlogit2_output = summary(md_mlogit2)
mlogit_ModelError <- as.data.frame(rbind(cbind(mlogit1_output$deviance,mlogit1_output$AIC)),cbind(mlogit2_output$deviance,mlogit2_output$AIC))
                                   
names(mlogit_ModelError) <- c("Deviance", "AIC")
print(mlogit_ModelError, digits = 3)

```

We observe that Model md_mlogit1 with all variables has highest McFadden
(0.18) and r2 scores and lowest deviance and AIC

### 3.7 Tiên đoán xác suất phân loại và dự đoán kết quả phân loại

Giả sử ta có nồng độ phần trăm các chất tan có trong rượu

```{r}
new_data_wine = head(classdatatest_scaled, 5)
new_data_wine
```

Với actual segmentation là

```{r}
new_data_wine$segmentation
```

Khi đó, ta tiên đoán xác suất phân loại như sau:

```{r}
new_data_wine = head(classdatatest_scaled, 5)
out_prob_mlogit_wine = predict(md_mlogit1, new_data_wine, type = "prob")
out_prob_mlogit_wine
```

Và nhóm dự đoán là

```{r}
out_class_mlogit_wine <- predict(md_mlogit1, newdata = new_data_wine, type = "class")
out_class_mlogit_wine
```

### 3.8 Đánh giá mô hình

Confusion matrix

```{r}
predictedML <- predict(md_mlogit1, classdatatest_scaled, na.action =na.pass, type="probs")
predicted_classML <- predict(md_mlogit1, classdatatest_scaled)

caret::confusionMatrix(as.factor(predicted_classML), as.factor(classdatatest_scaled$segmentation))
```

accuracy rate

```{r}
mean(as.character(predicted_classML) != as.character(classdatatest_scaled$segmentation))
```

Confusion Matrix gives that Multinominal Logist model with all variables
to have 58.74% accuracy and has around 41.26% accuracy rate
